{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from prepro import convert_idx, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/soham/data/squad/sample1k-HCVerifyAll.json', 'r') as f:\n",
    "    foo = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = np.load('final_train_pairs.npy').tolist()\n",
    "test_pairs = np.load('final_test_pairs.npy').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_count = lambda x: sum(map(lambda y: 1 if y[-2] == 'adv' else 0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train:', '%d,' % len(train_pairs), adv_count(train_pairs), 'of which are adversarial')\n",
    "print('Test:', '%d,' % len(test_pairs), adv_count(test_pairs), 'of which are adversarial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_word(word):\n",
    "    for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "        if each in word2idx_dict:\n",
    "            return word2idx_dict[each]\n",
    "    return 1\n",
    "\n",
    "def _get_char(char):\n",
    "    if char in char2idx_dict:\n",
    "        return char2idx_dict[char]\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/word2idx.json', 'r') as fh:\n",
    "    word2idx_dict = json.load(fh)\n",
    "\n",
    "with open('data/char2idx.json', 'r') as fh:\n",
    "    char2idx_dict = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_limit = 1000\n",
    "ques_limit = 100\n",
    "char_limit = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1337)\n",
    "random.shuffle(train_pairs)\n",
    "random.shuffle(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([tp[0] for tp in train_pairs]), len(set([tp[0] for tp in train_pairs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm data/og_check_*.tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.python_io.TFRecordWriter('data/og_check_train.tf')\n",
    "\n",
    "for id_, (id_but_ignore, context, ques, ans, start_end, tag, UNK) in enumerate(train_pairs):\n",
    "    context = context.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    context_tokens = word_tokenize(context)\n",
    "    context_chars = [list(token) for token in context_tokens]\n",
    "\n",
    "    ques = ques.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    ques_tokens = word_tokenize(ques)\n",
    "    ques_chars = [list(token) for token in ques_tokens]\n",
    "\n",
    "    context_idxs = np.zeros([para_limit], dtype=np.int32)\n",
    "    context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
    "    ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n",
    "    ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
    "\n",
    "    for i, token in enumerate(context_tokens):\n",
    "        context_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(ques_tokens):\n",
    "        ques_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(context_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "            context_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "    for i, token in enumerate(ques_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "\n",
    "            ques_char_idxs[i, j] = _get_char(char)\n",
    "    \n",
    "    ### NEW PROC GOES HERE\n",
    "    bad_y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "    bad_y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "    \n",
    "    spans = convert_idx(context, context_tokens)\n",
    "    \n",
    "    if start_end is None:\n",
    "        bad_y1[0] = 1\n",
    "        bad_y2[0] = 1\n",
    "    else:\n",
    "        start, end = start_end\n",
    "        \n",
    "        answer_span = []\n",
    "        \n",
    "        for idx, span in enumerate(spans):\n",
    "            if not (end <= span[0] or start >= span[1]):\n",
    "                answer_span.append(idx)\n",
    "                \n",
    "        bad_y1[answer_span[0]] = 1\n",
    "        bad_y2[answer_span[-1]] = 1\n",
    "        \n",
    "    ### NEW PROC GOES HERE\n",
    "    y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "    y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "    \n",
    "    start, end = ans[-1]['answer_start'], ans[-1]['answer_start'] + len(ans[-1]['text'])\n",
    "\n",
    "    answer_span = []\n",
    "\n",
    "    for idx, span in enumerate(spans):\n",
    "        if not (end <= span[0] or start >= span[1]):\n",
    "            answer_span.append(idx)\n",
    "\n",
    "    y1[answer_span[0]] = 1\n",
    "    y2[answer_span[-1]] = 1\n",
    "    \n",
    "    record = tf.train.Example(features=tf.train.Features(feature={\n",
    "                              \"context_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])),\n",
    "                              \"ques_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])),\n",
    "                              \"context_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])),\n",
    "                              \"ques_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])),\n",
    "                              \"y1\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])),\n",
    "                              \"y2\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])),\n",
    "                              \"id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[id_])),\n",
    "                              }))\n",
    "    \n",
    "    writer.write(record.SerializeToString())\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.python_io.TFRecordWriter('data/og_check_test.tf')\n",
    "meta_info = {}\n",
    "\n",
    "for id_, (id_but_ignore, context, ques, ans, start_end, tag, UNK) in enumerate(test_pairs):\n",
    "    context = context.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    context_tokens = word_tokenize(context)\n",
    "    context_chars = [list(token) for token in context_tokens]\n",
    "\n",
    "    ques = ques.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    ques_tokens = word_tokenize(ques)\n",
    "    ques_chars = [list(token) for token in ques_tokens]\n",
    "\n",
    "    context_idxs = np.zeros([para_limit], dtype=np.int32)\n",
    "    context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
    "    ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n",
    "    ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
    "\n",
    "    for i, token in enumerate(context_tokens):\n",
    "        context_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(ques_tokens):\n",
    "        ques_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(context_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "            context_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "    for i, token in enumerate(ques_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "\n",
    "            ques_char_idxs[i, j] = _get_char(char)\n",
    "    \n",
    "    ### NEW PROC GOES HERE\n",
    "    bad_y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "    bad_y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "    \n",
    "    spans = convert_idx(context, context_tokens)\n",
    "    \n",
    "    if start_end is None:\n",
    "        bad_y1[0] = 1\n",
    "        bad_y2[0] = 1\n",
    "    else:\n",
    "        start, end = start_end\n",
    "        \n",
    "        answer_span = []\n",
    "        \n",
    "        for idx, span in enumerate(spans):\n",
    "            if not (end <= span[0] or start >= span[1]):\n",
    "                answer_span.append(idx)\n",
    "                \n",
    "        bad_y1[answer_span[0]] = 1\n",
    "        bad_y2[answer_span[-1]] = 1\n",
    "    \n",
    "    ### NEW PROC GOES HERE\n",
    "    y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "    y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "    \n",
    "    start, end = ans[-1]['answer_start'], ans[-1]['answer_start'] + len(ans[-1]['text'])\n",
    "\n",
    "    answer_span = []\n",
    "\n",
    "    for idx, span in enumerate(spans):\n",
    "        if not (end <= span[0] or start >= span[1]):\n",
    "            answer_span.append(idx)\n",
    "\n",
    "    y1[answer_span[0]] = 1\n",
    "    y2[answer_span[-1]] = 1\n",
    "    \n",
    "    if id_but_ignore in set([foo['uuid'] for foo in meta_info.values()]):\n",
    "        print('hey', id_but_ignore)\n",
    "        break\n",
    "    \n",
    "    meta_info[id_] = {'answers': [a['text'] for a in ans], 'question': ques, 'spans': spans, 'context': context, 'uuid': id_but_ignore}\n",
    "    \n",
    "    record = tf.train.Example(features=tf.train.Features(feature={\n",
    "                              \"context_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])),\n",
    "                              \"ques_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])),\n",
    "                              \"context_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])),\n",
    "                              \"ques_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])),\n",
    "                              \"y1\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])),\n",
    "                              \"y2\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])),\n",
    "                              \"id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[id_])),\n",
    "                              }))\n",
    "    \n",
    "    writer.write(record.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "save('data/og_check_test_meta.json', meta_info, message='meta_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([a for a in map(lambda x: x['uuid'], meta_info.values())]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
