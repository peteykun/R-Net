{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from binary_model import BinaryModel\n",
    "from util import convert_tokens, get_batch_dataset, get_dataset\n",
    "import tensorflow as tf\n",
    "from config import flags\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_record_parser(config, is_test=True):\n",
    "    def parse(example):\n",
    "        para_limit = config.test_para_limit if is_test else config.para_limit\n",
    "        ques_limit = config.test_ques_limit if is_test else config.ques_limit\n",
    "        char_limit = config.char_limit\n",
    "        features = tf.parse_single_example(example,\n",
    "                                           features={\n",
    "                                               \"context_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"ques_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"context_char_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"ques_char_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"id\": tf.FixedLenFeature([], tf.int64),\n",
    "                                               \"tag\": tf.FixedLenFeature([], tf.string)\n",
    "                                           })\n",
    "        context_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"context_idxs\"], tf.int32), [para_limit])\n",
    "        ques_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"ques_idxs\"], tf.int32), [ques_limit])\n",
    "        context_char_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"context_char_idxs\"], tf.int32), [para_limit, char_limit])\n",
    "        ques_char_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"ques_char_idxs\"], tf.int32), [ques_limit, char_limit])\n",
    "        qa_id = features[\"id\"]\n",
    "        tag = tf.reshape(tf.decode_raw(\n",
    "            features[\"tag\"], tf.int32), [2])\n",
    "        return context_idxs, ques_idxs, context_char_idxs, ques_char_idxs, qa_id, tag\n",
    "    \n",
    "    return parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_string('f', 'give up already', 'who cares lol')\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval_file = 'data/binary_test_meta.json'\n",
    "test_record_file = 'data/binary_test.tf'\n",
    "\n",
    "with open(test_eval_file, \"r\") as fh:\n",
    "    eval_file = json.load(fh)\n",
    "\n",
    "meta = {'total': 1382}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.word_emb_file, \"r\") as fh:\n",
    "    word_mat = np.array(json.load(fh), dtype=np.float32)\n",
    "with open(config.char_emb_file, \"r\") as fh:\n",
    "    char_mat = np.array(json.load(fh), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = get_dataset(test_record_file, get_binary_record_parser(\n",
    "        config, is_test=True), config).make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/soham/NLU-Project/env/local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "model = BinaryModel(config, test_batch, word_mat, char_mat, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session(config=sess_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from log/model/../binary_model/badptr-savepoint-2700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint(config.save_dir + '/../binary_model'))\n",
    "sess.run(tf.assign(model.is_train, tf.constant(False, dtype=tf.bool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1382\n"
     ]
    }
   ],
   "source": [
    "total = meta['total']\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens(eval_file, qa_id, pred, target):\n",
    "    answer_dict = {}\n",
    "    remapped_dict = {}\n",
    "    for qid, p1, p2 in zip(qa_id, pred.argmax(1), target.argmax(1)):\n",
    "        uuid = eval_file[str(qid)][\"uuid\"]\n",
    "        \n",
    "        answer_dict[str(qid)] = p1 == p2\n",
    "        remapped_dict[uuid] = p1 == p2\n",
    "    return answer_dict, remapped_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "answer_dict = {}\n",
    "remapped_dict = {}\n",
    "\n",
    "for step in range(total // config.batch_size + 1):\n",
    "    qa_id, loss, yp1, yp2 = sess.run([model.qa_id, model.loss, model.prediction, model.y_target])\n",
    "    answer_dict_, remapped_dict_ = convert_tokens(eval_file, qa_id.tolist(), yp1, yp2)\n",
    "    answer_dict.update(answer_dict_)\n",
    "    remapped_dict.update(remapped_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import metric_max_over_ground_truths, exact_match_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_file, answer_dict, only=None):\n",
    "    f1 = exact_match = total = 0\n",
    "    for key, value in answer_dict.items():\n",
    "        if only == 'adv' and len(eval_file[key]['uuid'].split('-')) == 1:\n",
    "            continue\n",
    "        if only == 'orig' and len(eval_file[key]['uuid'].split('-')) > 1:\n",
    "            continue\n",
    "        total += 1\n",
    "        exact_match += 1 if value else 0\n",
    "    em = 100.0 * exact_match / total\n",
    "    return {'exact_match': em, 'count_true': exact_match, 'count_false': total-exact_match}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmutated data\n",
      "Accuracy: 85.20408163265306, True: 334, False: 58\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate(eval_file, answer_dict, only='orig')\n",
    "print(\"Unmutated data\")\n",
    "print(\"Accuracy: {}, True: {}, False: {}\".format(metrics['exact_match'], metrics['count_true'], metrics['count_false']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn = metrics['count_true']\n",
    "fp = metrics['count_false']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutated data\n",
      "Accuracy: 98.17997977755309, True: 971, False: 18\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate(eval_file, answer_dict, only='adv')\n",
    "print(\"Mutated data\")\n",
    "print(\"Accuracy: {}, True: {}, False: {}\".format(metrics['exact_match'], metrics['count_true'], metrics['count_false']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = metrics['count_true']\n",
    "fn = metrics['count_false']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall = $\\frac{TP}{TP + FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.17997977755309\n"
     ]
    }
   ],
   "source": [
    "R = tp/(tp+fn)\n",
    "print(R*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision = $\\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.36345966958211\n"
     ]
    }
   ],
   "source": [
    "P = tp/(tp+fp)\n",
    "print(P*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F_1$ Measure = $\\frac{2PR}{P+R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.23389494549059\n"
     ]
    }
   ],
   "source": [
    "F = (2*P*R)/(P+R)\n",
    "print(F*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = $\\frac{TP+TN}{TP+TN+FP+FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.2315484804631\n"
     ]
    }
   ],
   "source": [
    "acc = (960+287)/(meta['total']) * 100\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
