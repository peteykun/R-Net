{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/soham/data/squad/sample1k-HCVerifyAll.json', 'r') as f:\n",
    "    foo = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = np.load('final_train_pairs.npy').tolist()\n",
    "test_pairs = np.load('final_test_pairs.npy').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_count = lambda x: sum(map(lambda y: 1 if y[-2] == 'adv' else 0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train:', '%d,' % len(train_pairs), adv_count(train_pairs), 'of which are adversarial')\n",
    "print('Test:', '%d,' % len(test_pairs), adv_count(test_pairs), 'of which are adversarial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_word(word):\n",
    "    for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "        if each in word2idx_dict:\n",
    "            return word2idx_dict[each]\n",
    "    return 1\n",
    "\n",
    "def _get_char(char):\n",
    "    if char in char2idx_dict:\n",
    "        return char2idx_dict[char]\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/word2idx.json', 'r') as fh:\n",
    "    word2idx_dict = json.load(fh)\n",
    "\n",
    "with open('data/char2idx.json', 'r') as fh:\n",
    "    char2idx_dict = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_limit = 1000\n",
    "ques_limit = 100\n",
    "char_limit = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_pairs)\n",
    "random.shuffle(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm data/binary_t*.tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.python_io.TFRecordWriter('data/binary_train.tf')\n",
    "\n",
    "for id_, (id_but_ignore, context, ques, ans, UNK1, tag, UNK2) in enumerate(train_pairs):\n",
    "    context = context.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    context_tokens = word_tokenize(context)\n",
    "    context_chars = [list(token) for token in context_tokens]\n",
    "\n",
    "    ques = ques.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    ques_tokens = word_tokenize(ques)\n",
    "    ques_chars = [list(token) for token in ques_tokens]\n",
    "\n",
    "    context_idxs = np.zeros([para_limit], dtype=np.int32)\n",
    "    context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
    "    ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n",
    "    ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
    "\n",
    "    for i, token in enumerate(context_tokens):\n",
    "        context_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(ques_tokens):\n",
    "        ques_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(context_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "            context_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "    for i, token in enumerate(ques_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "\n",
    "            ques_char_idxs[i, j] = _get_char(char)\n",
    "    \n",
    "    tag_result = np.zeros([2], dtype=np.int32)\n",
    "    \n",
    "    if tag == 'adv':\n",
    "        tag_result[1] = 1\n",
    "    else:\n",
    "        assert tag == 'orig'\n",
    "        tag_result[0] = 1\n",
    "    \n",
    "    record = tf.train.Example(features=tf.train.Features(feature={\n",
    "                              \"context_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])),\n",
    "                              \"ques_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])),\n",
    "                              \"context_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])),\n",
    "                              \"ques_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])),\n",
    "                              \"id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[id_])),\n",
    "                              \"tag\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[tag_result.tostring()])),\n",
    "                              }))\n",
    "    \n",
    "    writer.write(record.SerializeToString())\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepro import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.python_io.TFRecordWriter('data/binary_test.tf')\n",
    "meta_info = {}\n",
    "\n",
    "for id_, (id_but_ignore, context, ques, ans, UNK1, tag, UNK2) in enumerate(test_pairs):\n",
    "    context = context.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    context_tokens = word_tokenize(context)\n",
    "    context_chars = [list(token) for token in context_tokens]\n",
    "\n",
    "    ques = ques.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    ques_tokens = word_tokenize(ques)\n",
    "    ques_chars = [list(token) for token in ques_tokens]\n",
    "\n",
    "    context_idxs = np.zeros([para_limit], dtype=np.int32)\n",
    "    context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
    "    ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n",
    "    ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
    "\n",
    "    for i, token in enumerate(context_tokens):\n",
    "        context_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(ques_tokens):\n",
    "        ques_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(context_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "            context_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "    for i, token in enumerate(ques_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "\n",
    "            ques_char_idxs[i, j] = _get_char(char)\n",
    "    \n",
    "    tag_result = np.zeros([2], dtype=np.int32)\n",
    "    \n",
    "    if tag == 'adv':\n",
    "        tag_result[1] = 1\n",
    "    else:\n",
    "        assert tag == 'orig'\n",
    "        tag_result[0] = 1\n",
    "    \n",
    "    meta_info[id_] = {'answers': [a['text'] for a in ans], 'spans': UNK1, 'context': context, 'uuid': id_but_ignore}\n",
    "    \n",
    "    record = tf.train.Example(features=tf.train.Features(feature={\n",
    "                              \"context_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])),\n",
    "                              \"ques_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])),\n",
    "                              \"context_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])),\n",
    "                              \"ques_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])),\n",
    "                              \"id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[id_])),\n",
    "                              \"tag\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[tag_result.tostring()])),\n",
    "                              }))\n",
    "    \n",
    "    writer.write(record.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "save('data/binary_test_meta.json', meta_info, message='meta_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([a for a in map(lambda x: x['uuid'], meta_info.values())]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
