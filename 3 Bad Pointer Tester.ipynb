{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from badptr_model import BadptrModel\n",
    "from util import convert_tokens, get_batch_dataset, get_dataset, get_record_parser\n",
    "import tensorflow as tf\n",
    "from config import flags\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_string('f', 'give up already', 'who cares lol')\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval_file = 'data/badptr_test_meta.json'\n",
    "test_record_file = 'data/badptr_test.tf'\n",
    "\n",
    "with open(test_eval_file, \"r\") as fh:\n",
    "    eval_file = json.load(fh)\n",
    "\n",
    "meta = {'total': 1382}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.word_emb_file, \"r\") as fh:\n",
    "    word_mat = np.array(json.load(fh), dtype=np.float32)\n",
    "with open(config.char_emb_file, \"r\") as fh:\n",
    "    char_mat = np.array(json.load(fh), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_record_parser(config, is_test=True):\n",
    "    def parse(example):\n",
    "        para_limit = config.test_para_limit if is_test else config.para_limit\n",
    "        ques_limit = config.test_ques_limit if is_test else config.ques_limit\n",
    "        char_limit = config.char_limit\n",
    "        features = tf.parse_single_example(example,\n",
    "                                           features={\n",
    "                                               \"context_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"ques_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"context_char_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"ques_char_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"id\": tf.FixedLenFeature([], tf.int64),\n",
    "                                               \"y1\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"y2\": tf.FixedLenFeature([], tf.string),\n",
    "                                           })\n",
    "        context_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"context_idxs\"], tf.int32), [para_limit])\n",
    "        ques_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"ques_idxs\"], tf.int32), [ques_limit])\n",
    "        context_char_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"context_char_idxs\"], tf.int32), [para_limit, char_limit])\n",
    "        ques_char_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"ques_char_idxs\"], tf.int32), [ques_limit, char_limit])\n",
    "        qa_id = features[\"id\"]\n",
    "        y1 = tf.reshape(tf.decode_raw(\n",
    "            features[\"y1\"], tf.float32), [para_limit])\n",
    "        y2 = tf.reshape(tf.decode_raw(\n",
    "            features[\"y2\"], tf.float32), [para_limit])\n",
    "        return context_idxs, ques_idxs, context_char_idxs, ques_char_idxs, qa_id, y1, y2\n",
    "    \n",
    "    return parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = get_dataset(test_record_file, get_binary_record_parser(\n",
    "        config, is_test=True), config).make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/soham/NLU-Project/env/local/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "model = BadptrModel(config, test_batch, word_mat, char_mat, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session(config=sess_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint(config.save_dir + '/../badptr_model'))\n",
    "sess.run(tf.assign(model.is_train, tf.constant(False, dtype=tf.bool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1382\n"
     ]
    }
   ],
   "source": [
    "total = meta['total']\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "answer_dict = {}\n",
    "remapped_dict = {}\n",
    "\n",
    "for step in range(total // config.batch_size + 1):\n",
    "    qa_id, loss, yp1, yp2 = sess.run([model.qa_id, model.loss, model.yp1, model.yp2])\n",
    "    answer_dict_, remapped_dict_ = convert_tokens(eval_file, qa_id.tolist(), yp1.tolist(), yp2.tolist())\n",
    "    answer_dict.update(answer_dict_)\n",
    "    remapped_dict.update(remapped_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import metric_max_over_ground_truths, exact_match_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_file, answer_dict, only=None):\n",
    "    f1 = exact_match = total = 0\n",
    "    for key, value in answer_dict.items():\n",
    "        if only == 'adv' and len(eval_file[key]['uuid'].split('-')) == 1:\n",
    "            continue\n",
    "        if only == 'orig' and len(eval_file[key]['uuid'].split('-')) > 1:\n",
    "            continue\n",
    "        total += 1\n",
    "        ground_truths = eval_file[key][\"answers\"]\n",
    "        prediction = value\n",
    "        exact_match += metric_max_over_ground_truths(\n",
    "            exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(f1_score,\n",
    "                                            prediction, ground_truths)\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmutated data\n",
      "Exact Match: 90.7103825136612, F1: 67.55399427530575\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate(eval_file, answer_dict, only='orig')\n",
    "print(\"Unmutated data\")\n",
    "print(\"Exact Match: {}, F1: {}\".format(metrics['exact_match'], metrics['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutated data\n",
      "Exact Match: 92.42125984251969, F1: 94.39447681441005\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate(eval_file, answer_dict, only='adv')\n",
    "print(\"Mutated data\")\n",
    "print(\"Exact Match: {}, F1: {}\".format(metrics['exact_match'], metrics['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall\n",
      "Exact Match: 91.96816208393632, F1: 87.28621588147794\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate(eval_file, answer_dict)\n",
    "print(\"Overall\")\n",
    "print(\"Exact Match: {}, F1: {}\".format(metrics['exact_match'], metrics['f1']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
