{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from prepro import convert_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/soham/data/squad/sample1k-HCVerifyAll.json', 'r') as f:\n",
    "    foo = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# i = 0; j = 0\n",
    "# original_paragraphs = {}\n",
    "# adversarial_paragraphs = {}\n",
    "# titles = set()\n",
    "\n",
    "# def strip_id(id_):\n",
    "#     return id_.split('-')[0]\n",
    "\n",
    "# for datum in foo['data']:\n",
    "#     titles.add(datum['title'])\n",
    "    \n",
    "#     for paragraph in datum['paragraphs']:\n",
    "#         context = paragraph['context']\n",
    "        \n",
    "#         for qa in paragraph['qas']:\n",
    "#             answers = qa['answers']\n",
    "#             question = qa['question']\n",
    "#             id_ = qa['id']\n",
    "            \n",
    "#             if strip_id(id_) == id_:\n",
    "#                 assert id_ not in original_paragraphs\n",
    "#                 original_paragraphs[id_] = (id_, datum['title'], context, question, answers)\n",
    "#                 i += 1\n",
    "#             else:\n",
    "#                 if strip_id(id_) not in adversarial_paragraphs:\n",
    "#                     adversarial_paragraphs[strip_id(id_)] = []\n",
    "                    \n",
    "#                 adversarial_paragraphs[strip_id(id_)] += [(context, question, answers)]\n",
    "#                 j += 1\n",
    "\n",
    "# print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1337)\n",
    "# train_titles = set(np.random.choice(list(titles), size=int(len(titles)*0.7), replace=False))\n",
    "# test_titles = titles - train_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(', '.join(list(train_titles)))\n",
    "# print()\n",
    "# print(', '.join(list(test_titles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3560 total:  \n",
    "\n",
    "* 2560 mutated\n",
    "* 1000 original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pairs = []\n",
    "# test_pairs = []\n",
    "\n",
    "# for key, (id_, title, orig_para, orig_ques, orig_ans) in original_paragraphs.items():\n",
    "#      #id_, title, orig_para, orig_ques, orig_answer = id_title_para_ques_ans\n",
    "    \n",
    "#     if title in train_titles:\n",
    "#         train_pairs += [(id_, orig_para, orig_ques, None, 'orig', title)]\n",
    "#     else:\n",
    "#         assert title in test_titles\n",
    "#         test_pairs += [(id_, orig_para, orig_ques, orig_ans, None, 'orig', title)]\n",
    "    \n",
    "#     if key in adversarial_paragraphs:\n",
    "#         for j, (para, ques, ans) in enumerate(adversarial_paragraphs[key]):\n",
    "#             injection = para[len(orig_para)+1:]\n",
    "            \n",
    "#             orig_sents = nltk.sent_tokenize(orig_para)\n",
    "            \n",
    "#             i = np.random.randint(0, len(orig_sents) + 1)\n",
    "#             adv_para = ' '.join(orig_sents[:i] + [injection] + orig_sents[i:])\n",
    "            \n",
    "#             if i == 0:\n",
    "#                 start = 0\n",
    "#             else:\n",
    "#                 start = len(' '.join(orig_sents[:i])) + 1\n",
    "            \n",
    "#             end = start + len(injection)\n",
    "#             die_counter = 0\n",
    "#             for j in range(len(ans)):\n",
    "#                 if start <= ans[j]['answer_start']:\n",
    "#                     new_start = ans[j]['answer_start'] + len(injection) - 1\n",
    "#                     pass #ans[j]['answer_start'] += len(injection)\n",
    "#                 else:\n",
    "#                     new_start = ans[j]['answer_start']\n",
    "                \n",
    "#                 die_counter = 0\n",
    "                \n",
    "#                 while not adv_para[new_start:].startswith(ans[j]['text']):\n",
    "#                     new_start += 1\n",
    "#                     die_counter += 1\n",
    "                    \n",
    "#                     if die_counter == 9999:\n",
    "#                         break\n",
    "                \n",
    "#                 if die_counter == 9999:\n",
    "#                     break\n",
    "                \n",
    "#                 assert adv_para[new_start:].startswith(ans[j]['text']), (adv_para[new_start:], ans[j]['text'])\n",
    "#                 ans[j]['answer_start'] = new_start\n",
    "            \n",
    "#             if die_counter == 9999:\n",
    "#                 print('YOU DIED')\n",
    "#                 print(adv_para)\n",
    "#                 print(orig_para)\n",
    "#                 print()\n",
    "#                 continue\n",
    "            \n",
    "#             if title in train_titles:\n",
    "#                 if True: #np.random.randint(3, size=1)[0] == 1:\n",
    "#                     train_pairs += [('%s-mut%d' % (id_, j), adv_para, orig_ques, (start, end), 'adv', title)]\n",
    "#             else:\n",
    "#                 assert title in test_titles\n",
    "#                 if True: #np.random.randint(3, size=1)[0] == 1:\n",
    "#                     test_pairs += [('%s-mut%d' % (id_, j), adv_para, orig_ques, ans, (start, end), 'adv', title)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = np.load('final_train_pairs.npy').tolist()\n",
    "test_pairs = np.load('final_test_pairs.npy').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_count = lambda x: sum(map(lambda y: 1 if y[-2] == 'adv' else 0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2178, 1570 of which are adversarial\n",
      "Test: 1381, 989 of which are adversarial\n"
     ]
    }
   ],
   "source": [
    "print('Train:', '%d,' % len(train_pairs), adv_count(train_pairs), 'of which are adversarial')\n",
    "print('Test:', '%d,' % len(test_pairs), adv_count(test_pairs), 'of which are adversarial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_word(word):\n",
    "    for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "        if each in word2idx_dict:\n",
    "            return word2idx_dict[each]\n",
    "    return 1\n",
    "\n",
    "def _get_char(char):\n",
    "    if char in char2idx_dict:\n",
    "        return char2idx_dict[char]\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/word2idx.json', 'r') as fh:\n",
    "    word2idx_dict = json.load(fh)\n",
    "\n",
    "with open('data/char2idx.json', 'r') as fh:\n",
    "    char2idx_dict = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_limit = 1000\n",
    "ques_limit = 100\n",
    "char_limit = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_pairs)\n",
    "random.shuffle(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm data/badptr_t*.tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.python_io.TFRecordWriter('data/badptr_train.tf')\n",
    "\n",
    "for id_, (id_but_ignore, context, ques, ans, start_end, tag, title) in enumerate(train_pairs):\n",
    "    context = context.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    context_tokens = word_tokenize(context)\n",
    "    context_chars = [list(token) for token in context_tokens]\n",
    "\n",
    "    ques = ques.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    ques_tokens = word_tokenize(ques)\n",
    "    ques_chars = [list(token) for token in ques_tokens]\n",
    "\n",
    "    context_idxs = np.zeros([para_limit], dtype=np.int32)\n",
    "    context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
    "    ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n",
    "    ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
    "\n",
    "    for i, token in enumerate(context_tokens):\n",
    "        context_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(ques_tokens):\n",
    "        ques_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(context_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "            context_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "    for i, token in enumerate(ques_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "\n",
    "            ques_char_idxs[i, j] = _get_char(char)\n",
    "    \n",
    "    ### NEW PROC GOES HERE\n",
    "    y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "    y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "    \n",
    "    if start_end is None:\n",
    "        y1[0] = 1\n",
    "        y2[0] = 1\n",
    "    else:\n",
    "        spans = convert_idx(context, context_tokens)\n",
    "        start, end = start_end\n",
    "        \n",
    "        answer_span = []\n",
    "        \n",
    "        for idx, span in enumerate(spans):\n",
    "            if not (end <= span[0] or start >= span[1]):\n",
    "                answer_span.append(idx)\n",
    "                \n",
    "        y1[answer_span[0]] = 1\n",
    "        y2[answer_span[-1]] = 1\n",
    "    \n",
    "    record = tf.train.Example(features=tf.train.Features(feature={\n",
    "                              \"context_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])),\n",
    "                              \"ques_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])),\n",
    "                              \"context_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])),\n",
    "                              \"ques_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])),\n",
    "                              \"id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[id_])),\n",
    "                              \"y1\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])),\n",
    "                              \"y2\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])),\n",
    "                              }))\n",
    "    \n",
    "    writer.write(record.SerializeToString())\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepro import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving meta_info...\n"
     ]
    }
   ],
   "source": [
    "writer = tf.python_io.TFRecordWriter('data/badptr_test.tf')\n",
    "classic_meta_info = {}\n",
    "meta_info = {}\n",
    "\n",
    "for id_, (id_but_ignore, context, ques, ans, start_end, tag, title) in enumerate(test_pairs):\n",
    "    context = context.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    context_tokens = word_tokenize(context)\n",
    "    context_chars = [list(token) for token in context_tokens]\n",
    "\n",
    "    ques = ques.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    ques_tokens = word_tokenize(ques)\n",
    "    ques_chars = [list(token) for token in ques_tokens]\n",
    "\n",
    "    context_idxs = np.zeros([para_limit], dtype=np.int32)\n",
    "    context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
    "    ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n",
    "    ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
    "\n",
    "    for i, token in enumerate(context_tokens):\n",
    "        context_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(ques_tokens):\n",
    "        ques_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(context_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "            context_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "    for i, token in enumerate(ques_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "\n",
    "            ques_char_idxs[i, j] = _get_char(char)\n",
    "    \n",
    "    ### NEW PROC GOES HERE\n",
    "    y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "    y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "    spans = convert_idx(context, context_tokens)\n",
    "    \n",
    "    if start_end is None:\n",
    "        y1[0] = 1\n",
    "        y2[0] = 1\n",
    "    else:\n",
    "        start, end = start_end\n",
    "        \n",
    "        answer_span = []\n",
    "        \n",
    "        for idx, span in enumerate(spans):\n",
    "            if not (end <= span[0] or start >= span[1]):\n",
    "                answer_span.append(idx)\n",
    "                \n",
    "        y1[answer_span[0]] = 1\n",
    "        y2[answer_span[-1]] = 1\n",
    "    \n",
    "    if start_end is None:\n",
    "        new_answer_ = context[spans[0][0]:spans[0][1]]\n",
    "    else:\n",
    "        new_answer_ = context[spans[answer_span[0]][0]:spans[answer_span[-1]][1]]\n",
    "    \n",
    "    meta_info[id_] = {'answers': [new_answer_], 'spans': spans, 'context': context, 'uuid': id_but_ignore}\n",
    "    classic_meta_info[id_] = {'answers': ans, 'spans': spans, 'context': context, 'ques': ques, 'tag': tag, 'title': title, 'id': id_}\n",
    "    \n",
    "    record = tf.train.Example(features=tf.train.Features(feature={\n",
    "                              \"context_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])),\n",
    "                              \"ques_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])),\n",
    "                              \"context_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])),\n",
    "                              \"ques_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])),\n",
    "                              \"id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[id_])),\n",
    "                              \"y1\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])),\n",
    "                              \"y2\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])),\n",
    "                              }))\n",
    "    \n",
    "    writer.write(record.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "save('data/badptr_test_meta.json', meta_info, message='meta_info')\n",
    "np.save('data/badptr_test_meta.npy', classic_meta_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1381"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([a for a in map(lambda x: x['uuid'], meta_info.values())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
