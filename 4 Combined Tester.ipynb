{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from combo_model import ComboModel\n",
    "from util import convert_tokens, get_batch_dataset, get_dataset, get_record_parser\n",
    "import tensorflow as tf\n",
    "from config import flags\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_string('f', 'give up already', 'who cares lol')\n",
    "config = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval_file = 'data/combo_v2_test_meta.json'\n",
    "test_record_file = 'data/combo_v2_test.tf'\n",
    "\n",
    "with open(test_eval_file, \"r\") as fh:\n",
    "    eval_file = json.load(fh)\n",
    "\n",
    "meta = {'total': 1382}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.word_emb_file, \"r\") as fh:\n",
    "    word_mat = np.array(json.load(fh), dtype=np.float32)\n",
    "with open(config.char_emb_file, \"r\") as fh:\n",
    "    char_mat = np.array(json.load(fh), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_record_parser(config, is_test=True):\n",
    "    def parse(example):\n",
    "        para_limit = config.test_para_limit if is_test else config.para_limit\n",
    "        ques_limit = config.test_ques_limit if is_test else config.ques_limit\n",
    "        char_limit = config.char_limit\n",
    "        features = tf.parse_single_example(example,\n",
    "                                           features={\n",
    "                                               \"context_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"ques_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"context_char_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"ques_char_idxs\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"id\": tf.FixedLenFeature([], tf.int64),\n",
    "                                               \"bad_y1\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"bad_y2\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"y1\": tf.FixedLenFeature([], tf.string),\n",
    "                                               \"y2\": tf.FixedLenFeature([], tf.string),\n",
    "                                           })\n",
    "        context_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"context_idxs\"], tf.int32), [para_limit])\n",
    "        ques_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"ques_idxs\"], tf.int32), [ques_limit])\n",
    "        context_char_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"context_char_idxs\"], tf.int32), [para_limit, char_limit])\n",
    "        ques_char_idxs = tf.reshape(tf.decode_raw(\n",
    "            features[\"ques_char_idxs\"], tf.int32), [ques_limit, char_limit])\n",
    "        qa_id = features[\"id\"]\n",
    "        bad_y1 = tf.reshape(tf.decode_raw(\n",
    "            features[\"bad_y1\"], tf.float32), [para_limit])\n",
    "        bad_y2 = tf.reshape(tf.decode_raw(\n",
    "            features[\"bad_y2\"], tf.float32), [para_limit])\n",
    "        y1 = tf.reshape(tf.decode_raw(\n",
    "            features[\"y1\"], tf.float32), [para_limit])\n",
    "        y2 = tf.reshape(tf.decode_raw(\n",
    "            features[\"y2\"], tf.float32), [para_limit])\n",
    "        return context_idxs, ques_idxs, context_char_idxs, ques_char_idxs, qa_id, bad_y1, bad_y2, y1, y2\n",
    "    \n",
    "    return parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = get_dataset('data/combo_v2_test.tf', get_binary_record_parser(\n",
    "        config, is_test=True), config).make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = ComboModel(config, test_batch, word_mat, char_mat, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session(config=sess_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(var_list=[v for v in tf.global_variables() if not v.name.startswith('encoding_1/') and not v.name.startswith('badptr') and '/Adam' not in v.name and 'beta' not in v.name])\n",
    "saver2 = tf.train.Saver()\n",
    "\n",
    "#### Restore trained bad pointer model!!\n",
    "if True:\n",
    "    saver2.restore(sess, tf.train.latest_checkpoint('log/combo_model'))\n",
    "else:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(config.save_dir))\n",
    "    sess.run(model.assign_trick_ops)\n",
    "\n",
    "sess.run(tf.assign(model.is_train, tf.constant(False, dtype=tf.bool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_1_vars = sorted([(v.name, v) for v in tf.global_variables() if v.name.startswith('encoding_1/Variable')])\n",
    "encoding_vars = sorted([(v.name, v) for v in tf.global_variables() if v.name.startswith('encoding/Variable')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n1, e1v), (n, ev) in zip(encoding_1_vars, encoding_vars):\n",
    "    assert n1.split('/')[1] == n.split('/')[1]\n",
    "    e1v_val, ev_val = sess.run([e1v, ev])\n",
    "    assert np.linalg.norm(e1v_val - ev_val) < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total = meta['total']\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens(eval_file, qa_id, pp1, pp2, bad_pp1, bad_pp2):\n",
    "    answer_dict = {}\n",
    "    remapped_dict = {}\n",
    "    for qid, p1, p2, bad_p1, bad_p2 in zip(qa_id, pp1, pp2, bad_pp1, bad_pp2):\n",
    "        context = eval_file[str(qid)][\"context\"]\n",
    "        spans = [z for z in eval_file[str(qid)][\"spans\"]]\n",
    "        uuid = eval_file[str(qid)][\"uuid\"]\n",
    "        start_idx = spans[p1][0]\n",
    "        end_idx = spans[p2][1]\n",
    "        \n",
    "        if bad_p1 != 0 or bad_p2 != 0:\n",
    "            for q in range(bad_p1, bad_p2+1):\n",
    "                 del spans[bad_p1]\n",
    "        \n",
    "        start_idx = spans[p1][0]\n",
    "        end_idx = spans[p2][1]\n",
    "        \n",
    "        answer_dict[str(qid)] = context[start_idx: end_idx]\n",
    "        remapped_dict[uuid] = context[start_idx: end_idx]\n",
    "    return answer_dict, remapped_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "answer_dict = {}\n",
    "remapped_dict = {}\n",
    "\n",
    "for step in range(total // config.batch_size + 1):\n",
    "    qa_id, loss, yp1, yp2, bad_yp1, bad_yp2 = sess.run([model.qa_id, model.loss, model.yp1, model.yp2,\n",
    "                                                        model.bad_yp1, model.bad_yp2])\n",
    "    answer_dict_, remapped_dict_ = convert_tokens(eval_file, qa_id.tolist(), yp1.tolist(), yp2.tolist(),\n",
    "                                                  bad_yp1.tolist(), bad_yp2.tolist())\n",
    "    answer_dict.update(answer_dict_)\n",
    "    remapped_dict.update(remapped_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import metric_max_over_ground_truths, exact_match_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_file, answer_dict, only=None):\n",
    "    f1 = exact_match = total = 0\n",
    "    for key, value in answer_dict.items():\n",
    "        if only == 'adv' and len(eval_file[key]['uuid'].split('-')) == 1:\n",
    "            continue\n",
    "        if only == 'orig' and len(eval_file[key]['uuid'].split('-')) > 1:\n",
    "            continue\n",
    "        total += 1\n",
    "        ground_truths = eval_file[key][\"answers\"]\n",
    "        prediction = value\n",
    "        exact_match += metric_max_over_ground_truths(\n",
    "            exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(f1_score,\n",
    "                                            prediction, ground_truths)\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate(eval_file, answer_dict, only='orig')\n",
    "print(\"Unmutated data\")\n",
    "print(\"Exact Match: {}, F1: {}\".format(metrics['exact_match'], metrics['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = evaluate(eval_file, answer_dict, only='adv')\n",
    "print(\"Mutated data\")\n",
    "print(\"Exact Match: {}, F1: {}\".format(metrics['exact_match'], metrics['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate(eval_file, answer_dict)\n",
    "print(\"Overall\")\n",
    "print(\"Exact Match: {}, F1: {}\".format(metrics['exact_match'], metrics['f1']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
