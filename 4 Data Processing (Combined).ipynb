{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from prepro import convert_idx, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/soham/data/squad/sample1k-HCVerifyAll.json', 'r') as f:\n",
    "    foo = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "meh = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 2560\n"
     ]
    }
   ],
   "source": [
    "i = 0; j = 0\n",
    "original_paragraphs = {}\n",
    "adversarial_paragraphs = {}\n",
    "titles = set()\n",
    "\n",
    "def strip_id(id_):\n",
    "    return id_.split('-')[0]\n",
    "\n",
    "for datum in foo['data']:\n",
    "    titles.add(datum['title'])\n",
    "    \n",
    "    for paragraph in datum['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        \n",
    "        for qa in paragraph['qas']:\n",
    "            answers = qa['answers']\n",
    "            question = qa['question']\n",
    "            id_ = qa['id']\n",
    "            \n",
    "            if strip_id(id_) == id_:\n",
    "                assert id_ not in original_paragraphs\n",
    "                original_paragraphs[id_] = (id_, datum['title'], context, question, answers)\n",
    "                i += 1\n",
    "            else:\n",
    "                if strip_id(id_) not in adversarial_paragraphs:\n",
    "                    adversarial_paragraphs[strip_id(id_)] = []\n",
    "                \n",
    "                #### YOU NEED TO UNCOMMENT THIS!! #####\n",
    "                adversarial_paragraphs[strip_id(id_)] += [(context, question, answers)]\n",
    "                j += 1\n",
    "\n",
    "print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([a for a in map(lambda x: x[0], original_paragraphs.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([a for a in map(lambda x: x[0], original_paragraphs.values())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "train_titles = set(np.random.choice(sorted(list(titles)), size=int(len(titles)*0.7), replace=False))\n",
    "test_titles = titles - train_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1973_oil_crisis, Amazon_rainforest, American_Broadcasting_Company, Apollo_program, Black_Death, Chloroplast, Computational_complexity_theory, Doctor_Who, Economic_inequality, European_Union_law, Force, French_and_Indian_War, Fresno,_California, Genghis_Khan, Geology, Huguenot, Imperialism, Intergovernmental_Panel_on_Climate_Change, Newcastle_upon_Tyne, Normans, Oxygen, Packet_switching, Pharmacy, Prime_number, Private_school, Rhine, Scottish_Parliament, Sky_(United_Kingdom), Southern_California, United_Methodist_Church, University_of_Chicago, Victoria_and_Albert_Museum, Warsaw\n",
      "\n",
      "Civil_disobedience, Construction, Ctenophora, Harvard_University, Immune_system, Islamism, Jacksonville,_Florida, Kenya, Martin_Luther, Nikola_Tesla, Steam_engine, Super_Bowl_50, Teacher, Victoria_(Australia), Yuan_dynasty\n"
     ]
    }
   ],
   "source": [
    "print(', '.join(sorted(list(train_titles))))\n",
    "print()\n",
    "print(', '.join(sorted(list(test_titles))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3560 total:  \n",
    "\n",
    "* 2560 mutated\n",
    "* 1000 original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOU DIED\n",
      "There were two kinds of X.25 networks. There were 7 kinds of X.75 networks. Some such as DATAPAC and TRANSPAC were there originally. Some such as DATAPAC and TRANSPAC were initially implemented with an X.25 external interface. Some older networks such as TELENET and TYMNET were modified to provide a X.25 host interface in addition to older host connection schemes. DATAPAC was developed by Bell Northern Research which was a joint venture of Bell Canada (a common carrier) and Northern Telecom (a telecommunications equipment supplier). Northern Telecom sold several DATAPAC clones to foreign PTTs including the Deutsche Bundespost. X.75 and X.121 allowed the interconnection of national X.25 networks. A user or host could call a host on a foreign network by including the DNIC of the remote network as part of the destination address. [citation needed]\n",
      "There were two kinds of X.25 networks. Some such as DATAPAC and TRANSPAC were initially implemented with an X.25 external interface. Some older networks such as TELENET and TYMNET were modified to provide a X.25 host interface in addition to older host connection schemes. DATAPAC was developed by Bell Northern Research which was a joint venture of Bell Canada (a common carrier) and Northern Telecom (a telecommunications equipment supplier). Northern Telecom sold several DATAPAC clones to foreign PTTs including the Deutsche Bundespost. X.75 and X.121 allowed the interconnection of national X.25 networks. A user or host could call a host on a foreign network by including the DNIC of the remote network as part of the destination address.[citation needed]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pairs = []\n",
    "test_pairs = []\n",
    "\n",
    "for key, (id_, title, orig_para, orig_ques, orig_ans) in original_paragraphs.items():\n",
    "     #id_, title, orig_para, orig_ques, orig_answer = id_title_para_ques_ans\n",
    "    \n",
    "    if title in train_titles:\n",
    "        train_pairs += [(id_, orig_para, orig_ques, orig_ans, None, 'orig', title)]\n",
    "    else:\n",
    "        assert title in test_titles\n",
    "        test_pairs += [(id_, orig_para, orig_ques, orig_ans, None, 'orig', title)]\n",
    "    \n",
    "    if key in adversarial_paragraphs:\n",
    "        for j, (para, ques, ans) in enumerate(adversarial_paragraphs[key]):\n",
    "            injection = para[len(orig_para)+1:]\n",
    "            \n",
    "            orig_sents = nltk.sent_tokenize(orig_para)\n",
    "            \n",
    "            i = np.random.randint(0, len(orig_sents) + 1)\n",
    "            adv_para = ' '.join(orig_sents[:i] + [injection] + orig_sents[i:])\n",
    "            \n",
    "            if i == 0:\n",
    "                start = 0\n",
    "            else:\n",
    "                start = len(' '.join(orig_sents[:i])) + 1\n",
    "            \n",
    "            end = start + len(injection)\n",
    "            die_counter = 0\n",
    "            \n",
    "            for k in range(len(ans)):\n",
    "                if start <= ans[k]['answer_start']:\n",
    "                    new_start = ans[k]['answer_start'] + len(injection) - 1\n",
    "                    pass #ans[j]['answer_start'] += len(injection)\n",
    "                else:\n",
    "                    new_start = ans[k]['answer_start']\n",
    "                \n",
    "                die_counter = 0\n",
    "                \n",
    "                while not adv_para[new_start:].startswith(ans[k]['text']):\n",
    "                    new_start += 1\n",
    "                    die_counter += 1\n",
    "                    \n",
    "                    if die_counter == 9999:\n",
    "                        break\n",
    "                \n",
    "                if die_counter == 9999:\n",
    "                    break\n",
    "                \n",
    "                assert adv_para[new_start:].startswith(ans[k]['text']), (adv_para[new_start:], ans[k]['text'])\n",
    "                ans[k]['answer_start'] = new_start\n",
    "            \n",
    "            if die_counter == 9999:\n",
    "                print('YOU DIED')\n",
    "                print(adv_para)\n",
    "                print(orig_para)\n",
    "                print()\n",
    "                continue\n",
    "            \n",
    "            if title in train_titles:\n",
    "                if np.random.randint(2, size=1)[0] == 1:\n",
    "                    train_pairs += [('%s-mut%d' % (id_, j), adv_para, orig_ques, ans, (start, end), 'adv', title)]\n",
    "            else:\n",
    "                assert title in test_titles\n",
    "                if np.random.randint(2, size=1)[0] == 1:\n",
    "                    test_pairs += [('%s-mut%d' % (id_, j), adv_para, orig_ques, ans, (start, end), 'adv', title)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('final_train_pairs.npy', train_pairs)\n",
    "np.save('final_test_pairs.npy', test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_count = lambda x: sum(map(lambda y: 1 if y[-2] == 'adv' else 0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1348, 740 of which are adversarial\n",
      "Test: 886, 494 of which are adversarial\n"
     ]
    }
   ],
   "source": [
    "print('Train:', '%d,' % len(train_pairs), adv_count(train_pairs), 'of which are adversarial')\n",
    "print('Test:', '%d,' % len(test_pairs), adv_count(test_pairs), 'of which are adversarial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_word(word):\n",
    "    for each in (word, word.lower(), word.capitalize(), word.upper()):\n",
    "        if each in word2idx_dict:\n",
    "            return word2idx_dict[each]\n",
    "    return 1\n",
    "\n",
    "def _get_char(char):\n",
    "    if char in char2idx_dict:\n",
    "        return char2idx_dict[char]\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/word2idx.json', 'r') as fh:\n",
    "    word2idx_dict = json.load(fh)\n",
    "\n",
    "with open('data/char2idx.json', 'r') as fh:\n",
    "    char2idx_dict = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_limit = 1000\n",
    "ques_limit = 100\n",
    "char_limit = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1337)\n",
    "random.shuffle(train_pairs)\n",
    "random.shuffle(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm data/combo_v2_*.tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.python_io.TFRecordWriter('data/combo_v2_train.tf')\n",
    "\n",
    "for id_, (id_but_ignore, context, ques, ans, start_end, tag, topic) in enumerate(train_pairs):\n",
    "    context = context.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    context_tokens = word_tokenize(context)\n",
    "    context_chars = [list(token) for token in context_tokens]\n",
    "\n",
    "    ques = ques.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    ques_tokens = word_tokenize(ques)\n",
    "    ques_chars = [list(token) for token in ques_tokens]\n",
    "\n",
    "    context_idxs = np.zeros([para_limit], dtype=np.int32)\n",
    "    context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
    "    ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n",
    "    ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
    "\n",
    "    for i, token in enumerate(context_tokens):\n",
    "        context_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(ques_tokens):\n",
    "        ques_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(context_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "            context_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "    for i, token in enumerate(ques_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "\n",
    "            ques_char_idxs[i, j] = _get_char(char)\n",
    "    \n",
    "    ### NEW PROC GOES HERE\n",
    "    bad_y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "    bad_y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "    \n",
    "    spans = convert_idx(context, context_tokens)\n",
    "    \n",
    "    if start_end is None:\n",
    "        bad_y1[0] = 1\n",
    "        bad_y2[0] = 1\n",
    "    else:\n",
    "        start, end = start_end\n",
    "        \n",
    "        answer_span = []\n",
    "        \n",
    "        for idx, span in enumerate(spans):\n",
    "            if not (end <= span[0] or start >= span[1]):\n",
    "                answer_span.append(idx)\n",
    "                \n",
    "        bad_y1[answer_span[0]] = 1\n",
    "        bad_y2[answer_span[-1]] = 1\n",
    "        \n",
    "    ### NEW PROC GOES HERE\n",
    "    y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "    y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "    \n",
    "    start, end = ans[-1]['answer_start'], ans[-1]['answer_start'] + len(ans[-1]['text'])\n",
    "\n",
    "    answer_span = []\n",
    "\n",
    "    for idx, span in enumerate(spans):\n",
    "        if not (end <= span[0] or start >= span[1]):\n",
    "            answer_span.append(idx)\n",
    "\n",
    "    y1[answer_span[0]] = 1\n",
    "    y2[answer_span[-1]] = 1\n",
    "    \n",
    "    record = tf.train.Example(features=tf.train.Features(feature={\n",
    "                              \"context_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])),\n",
    "                              \"ques_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])),\n",
    "                              \"context_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])),\n",
    "                              \"ques_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])),\n",
    "                              \"id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[id_])),\n",
    "                              \"bad_y1\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[bad_y1.tostring()])),\n",
    "                              \"bad_y2\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[bad_y2.tostring()])),\n",
    "                              \"y1\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])),\n",
    "                              \"y2\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])),\n",
    "                              }))\n",
    "    \n",
    "    writer.write(record.SerializeToString())\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving meta_info...\n"
     ]
    }
   ],
   "source": [
    "writer = tf.python_io.TFRecordWriter('data/combo_v2_test.tf')\n",
    "classic_meta_info = {}\n",
    "meta_info = {}\n",
    "\n",
    "for id_, (id_but_ignore, context, ques, ans, start_end, tag, topic) in enumerate(test_pairs):\n",
    "    context = context.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    context_tokens = word_tokenize(context)\n",
    "    context_chars = [list(token) for token in context_tokens]\n",
    "\n",
    "    ques = ques.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
    "    ques_tokens = word_tokenize(ques)\n",
    "    ques_chars = [list(token) for token in ques_tokens]\n",
    "\n",
    "    context_idxs = np.zeros([para_limit], dtype=np.int32)\n",
    "    context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n",
    "    ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n",
    "    ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n",
    "\n",
    "    for i, token in enumerate(context_tokens):\n",
    "        context_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(ques_tokens):\n",
    "        ques_idxs[i] = _get_word(token)\n",
    "\n",
    "    for i, token in enumerate(context_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "            context_char_idxs[i, j] = _get_char(char)\n",
    "\n",
    "    for i, token in enumerate(ques_chars):\n",
    "        for j, char in enumerate(token):\n",
    "            if j == char_limit:\n",
    "                break\n",
    "\n",
    "            ques_char_idxs[i, j] = _get_char(char)\n",
    "    \n",
    "    ### NEW PROC GOES HERE\n",
    "    bad_y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "    bad_y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "    \n",
    "    spans = convert_idx(context, context_tokens)\n",
    "    \n",
    "    if start_end is None:\n",
    "        bad_y1[0] = 1\n",
    "        bad_y2[0] = 1\n",
    "    else:\n",
    "        start, end = start_end\n",
    "        \n",
    "        answer_span = []\n",
    "        \n",
    "        for idx, span in enumerate(spans):\n",
    "            if not (end <= span[0] or start >= span[1]):\n",
    "                answer_span.append(idx)\n",
    "                \n",
    "        bad_y1[answer_span[0]] = 1\n",
    "        bad_y2[answer_span[-1]] = 1\n",
    "    \n",
    "    ### NEW PROC GOES HERE\n",
    "    y1 = np.zeros([para_limit], dtype=np.float32)\n",
    "    y2 = np.zeros([para_limit], dtype=np.float32)\n",
    "    \n",
    "    start, end = ans[-1]['answer_start'], ans[-1]['answer_start'] + len(ans[-1]['text'])\n",
    "\n",
    "    answer_span = []\n",
    "\n",
    "    for idx, span in enumerate(spans):\n",
    "        if not (end <= span[0] or start >= span[1]):\n",
    "            answer_span.append(idx)\n",
    "\n",
    "    y1[answer_span[0]] = 1\n",
    "    y2[answer_span[-1]] = 1\n",
    "    \n",
    "    meta_info[id_] = {'answers': [a['text'] for a in ans], 'spans': spans, 'context': context, 'uuid': id_but_ignore}\n",
    "    classic_meta_info[id_] = {'answers': ans, 'spans': spans, 'context': context, 'ques': ques, 'tag': tag, 'title': title, 'uuid': id_but_ignore}\n",
    "    \n",
    "    record = tf.train.Example(features=tf.train.Features(feature={\n",
    "                              \"context_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])),\n",
    "                              \"ques_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])),\n",
    "                              \"context_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])),\n",
    "                              \"ques_char_idxs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])),\n",
    "                              \"id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[id_])),\n",
    "                              \"bad_y1\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[bad_y1.tostring()])),\n",
    "                              \"bad_y2\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[bad_y2.tostring()])),\n",
    "                              \"y1\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])),\n",
    "                              \"y2\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])),\n",
    "                              }))\n",
    "    \n",
    "    writer.write(record.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "save('data/combo_v2_test_meta.json', meta_info, message='meta_info')\n",
    "np.save('data/combo_v2_test_meta.npy', classic_meta_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1381"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([a for a in map(lambda x: x['uuid'], classic_meta_info.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1381"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([a for a in map(lambda x: x['uuid'], classic_meta_info.values())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1381\n"
     ]
    }
   ],
   "source": [
    "broken = 0\n",
    "total = 0\n",
    "\n",
    "for id_, (id_but_ignore, context, ques, ans, start_end, tag, topic) in enumerate(test_pairs):\n",
    "    for answer in ans:\n",
    "        if not context[answer['answer_start']:].startswith(answer['text']):\n",
    "            broken += 1\n",
    "        total += 1\n",
    "        break\n",
    "\n",
    "print(broken, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1381"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([a for a in map(lambda x: x['uuid'], meta_info.values())]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
